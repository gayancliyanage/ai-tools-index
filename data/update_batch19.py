#!/usr/bin/env python3
import json
from datetime import datetime

# Load the JSON
with open('tools.json', 'r') as f:
    data = json.load(f)

# Updates for tools 901-950
updates = {
    901: {
        "name": "ConsistentID",
        "url": "https://github.com/JackAILab/ConsistentID",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "China",
        "description": "Identity-preserving portrait generation with multimodal fine-grained prompts using single reference image",
        "pricing": "Open source",
        "features": ["Identity preservation", "Single image input", "Fine-grained control", "Portrait generation"],
        "social": {"github": 1500},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": True,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "arXiv paper Apr 2024. Outperforms previous methods in identity preservation across expression, pose, and style variations.",
        "dataQuality": "verified"
    },
    902: {
        "name": "InstantID",
        "url": "https://instantid.github.io",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "China",
        "description": "Zero-shot identity-preserving generation in seconds with single image, tuning-free approach",
        "pricing": "Open source",
        "features": ["Zero-shot generation", "ID preservation", "ControlNet compatible", "SDXL integration"],
        "social": {"github": 12000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": True,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "State-of-the-art tuning-free method. Works with SD1.5 and SDXL. InstantX Research, Jan 2024.",
        "dataQuality": "verified"
    },
    903: {
        "name": "MoA (Mixture of Attention)",
        "url": "https://github.com/moa-diffusion/moa",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Open Source",
        "description": "Mixture of Attention for subject-context generation with better identity preservation",
        "pricing": "Open source",
        "features": ["Attention mixing", "Subject preservation", "Context generation"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    904: {
        "name": "MS-Diffusion",
        "url": "https://github.com/MS-Diffusion/MS-Diffusion",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "China",
        "description": "Multi-subject zero-shot image personalization with layout guidance",
        "pricing": "Open source",
        "features": ["Multi-subject generation", "Layout guidance", "Zero-shot learning", "Cross-attention"],
        "social": {"github": 800},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "ICLR 2025. Alibaba Group internship project. First layout-guided multi-subject personalization.",
        "dataQuality": "verified"
    },
    905: {
        "name": "Subject-Diffusion",
        "url": "https://github.com/OPPO-Mente-Lab/Subject-Diffusion",
        "category": "Image Generation",
        "founded": "2023",
        "headquarters": "China",
        "description": "Open-domain personalized text-to-image generation without test-time fine-tuning",
        "pricing": "Open source",
        "features": ["Personalized generation", "No fine-tuning", "Open domain"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "OPPO Mente Lab. Subject-aware diffusion model.",
        "dataQuality": "estimated"
    },
    906: {
        "name": "Re-Imagen",
        "url": "https://arxiv.org/abs/2209.14491",
        "category": "Image Generation",
        "founded": "2022",
        "headquarters": "USA",
        "description": "Google's retrieval-augmented text-to-image generation model",
        "pricing": "Research only",
        "features": ["Retrieval augmented", "Text-to-image", "Knowledge grounding"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Google Research. Uses retrieved images for better grounding and diversity.",
        "dataQuality": "estimated"
    },
    907: {
        "name": "SuTI",
        "url": "https://arxiv.org/abs/2303.08084",
        "category": "Image Generation",
        "founded": "2023",
        "headquarters": "USA",
        "description": "Subject-driven Text-to-Image generation via apprenticeship learning",
        "pricing": "Research only",
        "features": ["Subject-driven", "Apprenticeship learning", "Few-shot"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Google Research. SuTI: Subject-driven Text-to-Image Generation.",
        "dataQuality": "estimated"
    },
    908: {
        "name": "FastComposer",
        "url": "https://github.com/mit-han-lab/fastcomposer",
        "category": "Image Generation",
        "founded": "2023",
        "headquarters": "USA",
        "description": "Tuning-free multi-subject image generation with localized attention",
        "pricing": "Open source",
        "features": ["Multi-subject", "Tuning-free", "Localized attention"],
        "social": {"github": 1200},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "MIT HAN Lab. Enables multi-subject generation without per-subject fine-tuning.",
        "dataQuality": "verified"
    },
    909: {
        "name": "BLIP-Diffusion",
        "url": "https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion",
        "category": "Image Generation",
        "founded": "2023",
        "headquarters": "San Francisco, USA",
        "description": "Pre-trained subject representation for controllable text-to-image generation by Salesforce",
        "pricing": "Open source",
        "features": ["Subject-driven generation", "ControlNet compatible", "Zero-shot", "Multimodal control"],
        "social": {"github": 10000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": True,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Salesforce AI Research. Part of LAVIS library. NeurIPS 2023. Works with SD models.",
        "dataQuality": "verified"
    },
    910: {
        "name": "Kosmos-G",
        "url": "https://github.com/xichenpan/Kosmos-G",
        "category": "Image Generation",
        "founded": "2023",
        "headquarters": "Redmond, USA",
        "description": "Microsoft's multimodal LLM for image generation in context",
        "pricing": "Open source",
        "features": ["MLLM-based", "Zero-shot subject-driven", "Interleaved input", "Multi-image prompts"],
        "social": {"github": 500},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Microsoft Research. ICLR 2024. Perceives general modalities and generates image conditions.",
        "dataQuality": "verified"
    },
    911: {
        "name": "UNIMO-G",
        "url": "https://arxiv.org/abs/2401.13388",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "China",
        "description": "Unified image generation model with multimodal conditional diffusion",
        "pricing": "Research only",
        "features": ["Unified generation", "Multimodal conditions", "Text and image input"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Baidu Research. Unified model for various image generation tasks.",
        "dataQuality": "estimated"
    },
    912: {
        "name": "Emu",
        "url": "https://github.com/baaivision/Emu",
        "category": "Image Generation",
        "founded": "2023",
        "headquarters": "Beijing, China",
        "description": "BAAI's large multimodal model for image and text generation",
        "pricing": "Open source",
        "features": ["Multimodal generation", "Autoregressive", "Image + text output"],
        "social": {"github": 3000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Beijing Academy of AI (BAAI). Also Emu by Meta for quality-focused image generation.",
        "dataQuality": "verified"
    },
    913: {
        "name": "Emu2",
        "url": "https://github.com/baaivision/Emu",
        "category": "Image Generation",
        "founded": "2023",
        "headquarters": "Beijing, China",
        "description": "Enhanced multimodal model with improved generation capabilities",
        "pricing": "Open source",
        "features": ["Multimodal generation", "Enhanced quality", "37B parameters"],
        "social": {"github": 3000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": True,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "BAAI. Emu2 with improved generation quality and 37B parameters.",
        "dataQuality": "verified"
    },
    914: {
        "name": "Emu3",
        "url": "https://ai.meta.com/emu",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Menlo Park, USA",
        "description": "Meta's native multimodal AI model for text-heavy image generation",
        "pricing": "Integrated in Meta AI",
        "features": ["Native multimodal", "Text rendering", "High quality", "Emu 3.5 latest"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Meta AI. Emu 3.5 excels at text-heavy images. Integrated into Meta AI apps.",
        "dataQuality": "verified"
    },
    915: {
        "name": "Chameleon",
        "url": "https://github.com/facebookresearch/chameleon",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Menlo Park, USA",
        "description": "Meta's mixed-modal early-fusion foundation model for text and image generation",
        "pricing": "Open source",
        "features": ["Mixed-modal", "Early fusion", "Text + image tokens", "Arbitrary sequences"],
        "social": {"github": 2000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Meta FAIR. May 2024. Treats images and text as tokens in unified architecture. Competitive on text-only benchmarks.",
        "dataQuality": "verified"
    },
    916: {
        "name": "Show-O",
        "url": "https://github.com/showlab/Show-o",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Singapore",
        "description": "Unified transformer for multimodal understanding and generation",
        "pricing": "Open source",
        "features": ["Unified model", "Understanding + generation", "Single transformer", "Discrete tokens"],
        "social": {"github": 1500},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "ShowLab. ICLR 2025. Single transformer unifies understanding and generation. Show-O2 released Jun 2025.",
        "dataQuality": "verified"
    },
    917: {
        "name": "Transfusion",
        "url": "https://arxiv.org/abs/2408.11039",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "USA",
        "description": "Unified model predicting next token and diffusing images",
        "pricing": "Research only",
        "features": ["Token prediction", "Image diffusion", "Unified training", "Multimodal"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Meta AI. ICLR 2025. Combines autoregressive and diffusion in single model.",
        "dataQuality": "estimated"
    },
    918: {
        "name": "MonoFormer",
        "url": "https://arxiv.org/abs/2403.15113",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Open Source",
        "description": "Unified transformer for understanding and generation",
        "pricing": "Research only",
        "features": ["Unified architecture", "Understanding", "Generation"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    919: {
        "name": "X-MDPT",
        "url": "https://arxiv.org/abs/2312.05243",
        "category": "Image Generation",
        "founded": "2023",
        "headquarters": "Open Source",
        "description": "Cross-modal discrete prompt tokenization for multimodal generation",
        "pricing": "Research only",
        "features": ["Discrete prompts", "Cross-modal", "Tokenization"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    920: {
        "name": "Infinity",
        "url": "https://arxiv.org/abs/2412.04431",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "China",
        "description": "Scaling bitwise autoregressive modeling for high-resolution image synthesis",
        "pricing": "Research only",
        "features": ["Bitwise AR", "High resolution", "Scalable"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "ByteDance. Infinite-vocabulary autoregressive transformer for images.",
        "dataQuality": "estimated"
    },
    921: {
        "name": "Lumina-T2X",
        "url": "https://github.com/Alpha-VLLM/Lumina-T2X",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "China",
        "description": "Flow-based large diffusion transformer for text-to-any modality generation",
        "pricing": "Open source",
        "features": ["Text-to-image", "Text-to-video", "Text-to-3D", "Text-to-audio", "DiT architecture"],
        "social": {"github": 2000},
        "imageGen": True,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Alpha-VLLM. Jun 2024. Unified framework for images, videos, 3D, and audio from text.",
        "dataQuality": "verified"
    },
    922: {
        "name": "Lumina-Next",
        "url": "https://github.com/Alpha-VLLM/Lumina-T2X",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "China",
        "description": "Next-generation Lumina model with improved capabilities",
        "pricing": "Open source",
        "features": ["Improved generation", "Multi-modal", "DiT architecture"],
        "social": {"github": 2000},
        "imageGen": True,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Part of Lumina family. Enhanced architecture and training.",
        "dataQuality": "verified"
    },
    923: {
        "name": "HunyuanDiT",
        "url": "https://github.com/Tencent-Hunyuan/HunyuanDiT",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Shenzhen, China",
        "description": "Tencent's bilingual diffusion transformer with fine-grained Chinese and English understanding",
        "pricing": "Open source",
        "features": ["Bilingual", "Chinese support", "DiT architecture", "Multi-resolution", "CLIP + T5 encoder"],
        "social": {"github": 4000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Tencent Hunyuan. First Chinese-native high-quality DiT model. V1.1 and V1.2 available.",
        "dataQuality": "verified"
    },
    924: {
        "name": "PixArt-Σ",
        "url": "https://github.com/PixArt-alpha/PixArt-sigma",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "China",
        "description": "Weak-to-strong training of DiT for 4K text-to-image generation",
        "pricing": "Open source",
        "features": ["4K generation", "Training efficient", "Token compression", "DiT architecture"],
        "social": {"github": 1500},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Mar 2024. Successor to PixArt-α. Key feature: efficient token compression for ultra-high-resolution.",
        "dataQuality": "verified"
    },
    925: {
        "name": "AuraFlow",
        "url": "https://github.com/fal-ai/auraflow",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "USA",
        "description": "Flow-based large scale text-to-image generation model",
        "pricing": "Open source",
        "features": ["Flow matching", "Large scale", "Open source"],
        "social": {"github": 1000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Fal.ai. Open-source flow-based alternative to proprietary models.",
        "dataQuality": "verified"
    },
    926: {
        "name": "Meissonic",
        "url": "https://github.com/viiika/Meissonic",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Open Source",
        "description": "Non-autoregressive masked image modeling for efficient image synthesis",
        "pricing": "Open source",
        "features": ["Masked modeling", "Non-autoregressive", "Efficient"],
        "social": {},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    927: {
        "name": "MARS",
        "url": "https://arxiv.org/abs/2401.05136",
        "category": "Video Generation",
        "founded": "2024",
        "headquarters": "Open Source",
        "description": "Mixture of Auto-Regressive models for fine-grained video generation",
        "pricing": "Research only",
        "features": ["Auto-regressive", "Fine-grained", "Video generation"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    928: {
        "name": "OmniGen",
        "url": "https://github.com/VectorSpaceLab/OmniGen",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Open Source",
        "description": "Unified diffusion model for image generation from multimodal prompts",
        "pricing": "Open source",
        "features": ["Unified model", "Multimodal prompts", "No extra modules", "Reasoning capability"],
        "social": {"github": 3000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": True,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "VectorSpaceLab. Sep 2024. Supports diverse tasks without additional modules. OmniGen-v1 on HuggingFace.",
        "dataQuality": "verified"
    },
    929: {
        "name": "Janus",
        "url": "https://github.com/deepseek-ai/Janus",
        "category": "Image Generation",
        "founded": "2024",
        "headquarters": "Beijing, China",
        "description": "DeepSeek's unified multimodal understanding and generation model",
        "pricing": "Open source",
        "features": ["Unified model", "Understanding + generation", "Decoupled visual encoding", "1.3B/7B models"],
        "social": {"github": 5000},
        "imageGen": True,
        "videoGen": False,
        "imageEditor": True,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "DeepSeek AI. Oct 2024. Janus-Pro-7B available. Decouples visual encoding for better performance.",
        "dataQuality": "verified"
    },
    930: {
        "name": "TokenFlow",
        "url": "https://github.com/omerbt/TokenFlow",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "Israel",
        "description": "Consistent diffusion features for consistent video editing",
        "pricing": "Open source",
        "features": ["Video editing", "Consistency", "No fine-tuning", "Text-driven"],
        "social": {"github": 2000},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": True,
        "note": "ICLR 2024. Enables consistent video editing using pre-trained text-to-image models.",
        "dataQuality": "verified"
    },
    931: {
        "name": "FateZero",
        "url": "https://github.com/ChenyangQiQi/FateZero",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "Singapore",
        "description": "Zero-shot text-based video editing via attention fusion",
        "pricing": "Open source",
        "features": ["Zero-shot editing", "Attention fusion", "Style editing", "Attribute editing"],
        "social": {"github": 1500},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": True,
        "note": "ICCV 2023 Oral. First zero-shot text-driven video style and local attribute editing.",
        "dataQuality": "verified"
    },
    932: {
        "name": "Video-P2P",
        "url": "https://github.com/ShaoTengLiu/Video-P2P",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "USA",
        "description": "Video editing with cross-attention control using prompt-to-prompt",
        "pricing": "Open source",
        "features": ["Prompt-to-prompt", "Cross-attention", "Video editing"],
        "social": {"github": 500},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": True,
        "dataQuality": "estimated"
    },
    933: {
        "name": "vid2vid-zero",
        "url": "https://github.com/baaivision/vid2vid-zero",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "Beijing, China",
        "description": "Zero-shot video-to-video translation using pre-trained diffusion models",
        "pricing": "Open source",
        "features": ["Zero-shot", "Video translation", "No training"],
        "social": {"github": 800},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": True,
        "note": "BAAI. Video editing without any training or optimization.",
        "dataQuality": "verified"
    },
    934: {
        "name": "Text2Video-Zero",
        "url": "https://github.com/Picsart-AI-Research/Text2Video-Zero",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "USA",
        "description": "Zero-shot text-to-video synthesis using pre-trained image diffusion models",
        "pricing": "Open source",
        "features": ["Zero-shot", "Text-to-video", "No video data needed"],
        "social": {"github": 4000},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Picsart AI Research. ICCV 2023. Generates videos without training on video data.",
        "dataQuality": "verified"
    },
    935: {
        "name": "ControlVideo",
        "url": "https://github.com/YBYBZhang/ControlVideo",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "China",
        "description": "Training-free controllable text-to-video generation",
        "pricing": "Open source",
        "features": ["ControlNet for video", "Training-free", "Temporal consistency"],
        "social": {"github": 1000},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Extends ControlNet to video generation without additional training.",
        "dataQuality": "verified"
    },
    936: {
        "name": "Control-A-Video",
        "url": "https://github.com/Weifeng-Chen/control-a-video",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "China",
        "description": "Controllable text-to-video generation with control signals",
        "pricing": "Open source",
        "features": ["Controllable generation", "Motion control", "Depth/edge guidance"],
        "social": {"github": 600},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    937: {
        "name": "VideoControlNet",
        "url": "https://github.com/videocontrolnet/videocontrolnet",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "Open Source",
        "description": "ControlNet adaptation for video generation with temporal consistency",
        "pricing": "Open source",
        "features": ["ControlNet", "Video", "Temporal modeling"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    938: {
        "name": "Gen-L-Video",
        "url": "https://github.com/G-U-N/Gen-L-Video",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "China",
        "description": "Multi-text conditioned long video generation",
        "pricing": "Open source",
        "features": ["Long video", "Multi-text", "Extended duration"],
        "social": {"github": 500},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Generates longer videos with multiple text prompts for different segments.",
        "dataQuality": "estimated"
    },
    939: {
        "name": "VLOGGER",
        "url": "https://enriccorona.github.io/vlogger/",
        "category": "Video Generation",
        "founded": "2024",
        "headquarters": "USA",
        "description": "Google's multimodal diffusion model for audio-driven human video generation",
        "pricing": "Research only",
        "features": ["Audio-driven", "Talking head", "Full body motion", "Diverse"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Google Research. CVPR 2024. Generates humans speaking from single image and audio.",
        "dataQuality": "verified"
    },
    940: {
        "name": "NUWA-XL",
        "url": "https://arxiv.org/abs/2303.12346",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "China",
        "description": "Microsoft's diffusion over diffusion for extremely long video generation",
        "pricing": "Research only",
        "features": ["Long video", "Hierarchical", "Minutes-long videos"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Microsoft Research Asia. Generates videos up to several minutes in length.",
        "dataQuality": "verified"
    },
    941: {
        "name": "SEINE",
        "url": "https://github.com/Vchitect/SEINE",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "China",
        "description": "Short-to-long video diffusion model for generative transition and prediction",
        "pricing": "Open source",
        "features": ["Scene transitions", "Video prediction", "Image-to-video"],
        "social": {"github": 800},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Vchitect. ICLR 2024. Specializes in scene transitions and video continuation.",
        "dataQuality": "verified"
    },
    942: {
        "name": "DynamiCrafter",
        "url": "https://github.com/Doubiiu/DynamiCrafter",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "China",
        "description": "Animating open-domain images with video diffusion priors",
        "pricing": "Open source",
        "features": ["Image animation", "Open domain", "Video diffusion", "Text-guided motion"],
        "social": {"github": 3000},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "ECCV 2024 Oral. Animates still images into dynamic videos based on text prompts.",
        "dataQuality": "verified"
    },
    943: {
        "name": "GenLV",
        "url": "https://arxiv.org/abs/2312.16551",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "Open Source",
        "description": "Generating long videos with multiple text prompts",
        "pricing": "Research only",
        "features": ["Long video", "Multi-prompt", "Extended generation"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    944: {
        "name": "VideoPoet",
        "url": "https://sites.research.google/videopoet/",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "Mountain View, USA",
        "description": "Google's large language model for zero-shot video and audio generation",
        "pricing": "Research only",
        "features": ["LLM-based", "Zero-shot", "Video + audio", "Multimodal input"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Google Research. Dec 2023. Uses LLM architecture instead of diffusion. Trained on 270M videos and 1B image-text pairs.",
        "dataQuality": "verified"
    },
    945: {
        "name": "VideoFactory",
        "url": "https://arxiv.org/abs/2305.10874",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "China",
        "description": "Swap attention in spatiotemporal for text-to-video generation",
        "pricing": "Research only",
        "features": ["Spatiotemporal attention", "Text-to-video", "High quality"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "dataQuality": "estimated"
    },
    946: {
        "name": "WALT",
        "url": "https://walt-video-diffusion.github.io/",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "USA",
        "description": "Google's window attention latent transformer for video generation",
        "pricing": "Research only",
        "features": ["Window attention", "Latent space", "Efficient", "High resolution"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Google Research. Efficient video generation with windowed attention mechanism.",
        "dataQuality": "verified"
    },
    947: {
        "name": "W.A.L.T.",
        "url": "https://walt-video-diffusion.github.io/",
        "category": "Video Generation",
        "founded": "2023",
        "headquarters": "USA",
        "description": "Window Attention Latent Transformer - same as WALT",
        "pricing": "Research only",
        "features": ["Window attention", "Latent space", "Efficient"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Same as WALT (id 946). Google Research video generation model.",
        "dataQuality": "verified"
    },
    948: {
        "name": "Lumiere",
        "url": "https://lumiere-video.github.io/",
        "category": "Video Generation",
        "founded": "2024",
        "headquarters": "Mountain View, USA",
        "description": "Google's text-to-video diffusion model with Space-Time U-Net",
        "pricing": "Research only",
        "features": ["Space-Time U-Net", "Realistic motion", "Full temporal generation", "Image-to-video"],
        "social": {},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Google Research. Jan 2024. Novel STUNet processes all frames at once for coherent motion.",
        "dataQuality": "verified"
    },
    949: {
        "name": "Stable Video 4D",
        "url": "https://stability.ai/news/stable-video-4d",
        "category": "3D/Animation",
        "founded": "2024",
        "headquarters": "London, UK",
        "description": "Stability AI's model for 4D video generation with multi-view consistency",
        "pricing": "Open source",
        "features": ["4D generation", "Multi-view", "Novel view synthesis", "Temporal consistency"],
        "social": {"github": 1000},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Stability AI. Jul 2024. Generates 8 novel-view videos from single input video. Built on SVD and SV3D.",
        "dataQuality": "verified"
    },
    950: {
        "name": "SV4D",
        "url": "https://huggingface.co/stabilityai/sv4d",
        "category": "3D/Animation",
        "founded": "2024",
        "headquarters": "London, UK",
        "description": "Stable Video 4D for multi-frame and multi-view consistent dynamic 3D content",
        "pricing": "Open source",
        "features": ["4D generation", "Multi-view video", "Dynamic 3D", "Novel view synthesis"],
        "social": {"github": 1000},
        "imageGen": False,
        "videoGen": True,
        "imageEditor": False,
        "imageCanvas": False,
        "videoTimeline": False,
        "note": "Same as Stable Video 4D (id 949). Stability AI's latent video diffusion model for 4D content.",
        "dataQuality": "verified"
    }
}

# Current timestamp
now = datetime.now().strftime("%Y-%m-%dT%H:%M:%S+08:00")

# Update tools
for tool in data['tools']:
    if tool['id'] in updates:
        update = updates[tool['id']]
        for key, value in update.items():
            tool[key] = value
        tool['lastVerified'] = now

# Update metadata
data['lastUpdated'] = datetime.now().isoformat()
data['researchProgress']['lastBatch'] = '901-950'
data['researchProgress']['nextBatch'] = '951-1000'
data['researchProgress']['lastResearched'] = datetime.now().isoformat()
if '851-900' not in data['researchProgress']['batchesCompleted']:
    data['researchProgress']['batchesCompleted'].append('851-900')
if '901-950' not in data['researchProgress']['batchesCompleted']:
    data['researchProgress']['batchesCompleted'].append('901-950')

# Write back
with open('tools.json', 'w') as f:
    json.dump(data, f, indent=2)

print(f"Updated {len(updates)} tools (901-950)")
print(f"Batches completed: {data['researchProgress']['batchesCompleted']}")
